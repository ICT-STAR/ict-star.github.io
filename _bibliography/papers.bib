---
---

@inproceedings{yang-etal-2024-fall,
    abbr = {EMNLP (Findings)},
    web = {https://yangwl.site/collapse-in-model-editing},
    title = "The Fall of {ROME}: Understanding the Collapse of {LLM}s in Model Editing",
    author = "Yang, Wanli  and
      Sun, Fei  and
      Tan, Jiajun  and
      Ma, Xinyu  and
      Su, Du  and
      Yin, Dawei  and
      Shen, Huawei",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.236/",
    html={https://aclanthology.org/2024.findings-emnlp.236},
    pdf={https://aclanthology.org/2024.findings-emnlp.236.pdf},
    github={https://github.com/WanliYoung/Collapse-in-Model-Editing},
    google_scholar_id={ziOE8S1-AIUC},
    doi = "10.18653/v1/2024.findings-emnlp.236",
    pages = "4079--4087",
    abstract = "Despite significant progress in model editing methods, their application in real-world scenarios remains challenging as they often cause large language models (LLMs) to collapse. Among them, ROME is particularly concerning, as it could disrupt LLMs with only a single edit. In this paper, we study the root causes of such collapse. Through extensive analysis, we identify two primary factors that contribute to the collapse: i) inconsistent handling of prefixed and unprefixed keys in the parameter update equation may result in very small denominators, causing excessively large parameter updates; ii) the subject of collapse cases is usually the first token, whose unprefixed key distribution significantly differs from the prefixed key distribution in autoregressive transformers, causing the aforementioned issue to materialize. To validate our findings, we propose a simple yet effective approach: uniformly using prefixed keys during editing phase and adding prefixes during testing phase to ensure the consistency between training and testing. The experimental results show that the proposed solution can prevent model collapse while maintaining the effectiveness of the edits."
}

@inproceedings{tan-etal-2024-blinded,
    abbr = {ACL},
    google_scholar_id={raTqNPD5sRQC},
    pdf={https://aclanthology.org/2024.acl-long.337/},
    html={https://tan-hexiang.github.io/Blinded_by_Generated_Contexts},
    github={https://github.com/Tan-Hexiang/RetrieveOrGenerated},
    title = "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?",
    author = "Tan, Hexiang  and
      Sun, Fei  and
      Yang, Wanli  and
      Wang, Yuanzhuo  and
      Cao, Qi  and
      Cheng, Xueqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.337/",
    doi = "10.18653/v1/2024.acl-long.337",
    pages = "6207--6227",
    abstract = "While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources.To investigate this, we formulate a systematic framework to identify whether LLMs' responses are attributed to either generated or retrieved contexts.To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer.Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information.We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs.Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs."
}
@inproceedings{yang-etal-2024-butterfly,
    abbr = {ACL (Findings)},
    google_scholar_id={kWvqk_afx_IC},
    html={https://yangwl.site/collapse-in-model-editing},
    github={https://github.com/WanliYoung/Collapse-in-Model-Editing},
    pdf={https://aclanthology.org/2024.findings-acl.322},
    title = "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
    author = "Yang, Wanli  and
      Sun, Fei  and
      Ma, Xinyu  and
      Liu, Xun  and
      Yin, Dawei  and
      Cheng, Xueqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.322/",
    doi = "10.18653/v1/2024.findings-acl.322",
    pages = "5419--5437",
    abstract = "Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating changes in an edited model`s perplexity are strongly correlated with its downstream task performances. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community`s attention to the potential risks inherent in model editing practices."
}

@inproceedings{tao-etal-2024-trust,
    abbr = {ACL (Findings)},
    google_scholar_id={w1MjKQ0l0TYC},
    html={https://aclanthology.org/2024.findings-acl.357},
    title = "When to Trust {LLM}s: Aligning Confidence with Response Quality",
    author = "Tao, Shuchang  and
      Yao, Liuyi  and
      Ding, Hanxing  and
      Xie, Yuexiang  and
      Cao, Qi  and
      Sun, Fei  and
      Gao, Jinyang  and
      Shen, Huawei  and
      Ding, Bolin",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.357/",
    doi = "10.18653/v1/2024.findings-acl.357",
    pages = "5984--5996",
    abstract = "Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness."
}

@inproceedings{tan-etal-2024-Unlink,
abbr = {WWW (short)},
google_scholar_id={1DsIQWDZLl8C},
html={https://dl.acm.org/doi/abs/10.1145/3589335.3651578},
author = {Tan, Jiajun and Sun, Fei and Qiu, Ruichen and Su, Du and Shen, Huawei},
title = {Unlink to Unlearn: Simplifying Edge Unlearning in GNNs},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651578},
doi = {10.1145/3589335.3651578},
abstract = {As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges yet suffer from over-forgetting, which means the unlearning process inadvertently removes excessive information beyond needed, leading to a significant performance decline for remaining edges. Our analysis identifies the loss functions of GNNDelete as the primary source of over-forgetting and also suggests that loss functions may be redundant for effective edge unlearning. Building on these insights, we simplify GNNDelete to develop Unlink to Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure. Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks, by upholding over 97.3\% of the retrained model's privacy protection capabilities and 99.8\% of its link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {489–492},
numpages = {4},
keywords = {graph neural networks, machine unlearning, over-forgetting},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{wu-2024-attack,
abbr = {RecSys},
google_scholar_id={An6A6Jpfc1oC},
pdf={https://dl.acm.org/doi/pdf/10.1145/3640457.3688148},
author = {Wu, Yunfan and Cao, Qi and Tao, Shuchang and Zhang, Kaike and Sun, Fei and Shen, Huawei},
title = {Accelerating the Surrogate Retraining for Poisoning Attacks against Recommender Systems},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688148},
doi = {10.1145/3640457.3688148},
abstract = {Recent studies have demonstrated the vulnerability of recommender systems to data poisoning attacks, where adversaries inject carefully crafted fake user interactions into the training data of recommenders to promote target items. Current attack methods involve iteratively retraining a surrogate recommender on the poisoned data with the latest fake users to optimize the attack. However, this repetitive retraining is highly time-consuming, hindering the efficient assessment and optimization of fake users. To mitigate this computational bottleneck and develop a more effective attack in an affordable time, we analyze the retraining process and find that a change in the representation of one user/item will cause a cascading effect through the user-item interaction graph. Under theoretical guidance, we introduce Gradient Passing (GP), a novel technique that explicitly passes gradients between interacted user-item pairs during backpropagation, thereby approximating the cascading effect and accelerating retraining. With just a single update, GP can achieve effects comparable to multiple original training iterations. Under the same number of retraining epochs, GP enables a closer approximation of the surrogate recommender to the victim. This more accurate approximation provides better guidance for optimizing fake users, ultimately leading to enhanced data poisoning attacks. Extensive experiments on real-world datasets demonstrate the efficiency and effectiveness of our proposed GP.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {701–711},
numpages = {11},
keywords = {Adversarial Learning, Poisoning Attacks, Recommender Systems},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{zhang-2024-adv,
abbr = {RecSys},
google_scholar_id={2l5NCbZemmgC},
pdf={https://dl.acm.org/doi/pdf/10.1145/3640457.3688120},
author = {Zhang, Kaike and Cao, Qi and Wu, Yunfan and Sun, Fei and Shen, Huawei and Cheng, Xueqi},
title = {Improving the Shortest Plank: Vulnerability-Aware Adversarial Training for Robust Recommender System},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688120},
doi = {10.1145/3640457.3688120},
abstract = {Recommender systems play a pivotal role in mitigating information overload in various fields. Nonetheless, the inherent openness of these systems introduces vulnerabilities, allowing attackers to insert fake users into the system’s training data to skew the exposure of certain items, known as poisoning attacks. Adversarial training has emerged as a notable defense mechanism against such poisoning attacks within recommender systems. Existing adversarial training methods apply perturbations of the same magnitude across all users to enhance system robustness against attacks. Yet, in reality, we find that attacks often affect only a subset of users who are vulnerable. These perturbations of indiscriminate magnitude make it difficult to balance effective protection for vulnerable users without degrading recommendation quality for those who are not affected. To address this issue, our research delves into understanding user vulnerability. Considering that poisoning attacks pollute the training data, we note that the higher degree to which a recommender system fits users’ training data correlates with an increased likelihood of users incorporating attack information, indicating their vulnerability. Leveraging these insights, we introduce the Vulnerability-aware Adversarial Training (VAT), designed to defend against poisoning attacks in recommender systems. VAT employs a novel vulnerability-aware function to estimate users’ vulnerability based on the degree to which the system fits them. Guided by this estimation, VAT applies perturbations of adaptive magnitude to each user, not only reducing the success ratio of attacks but also preserving, and potentially enhancing, the quality of recommendations. Comprehensive experiments confirm VAT’s superior defensive capabilities across different recommendation models and against various types of attacks.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {680–689},
numpages = {10},
keywords = {Adversarial Training, Poisoning Attack, Robust Recommender System},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{zhang-2024-lorec,
abbr = {SIGIR},
google_scholar_id={QUX0mv85b1cC},
pdf={https://dl.acm.org/doi/pdf/10.1145/3626772.3657684},
author = {Zhang, Kaike and Cao, Qi and Wu, Yunfan and Sun, Fei and Shen, Huawei and Cheng, Xueqi},
title = {LoRec: Combating Poisons with Large Language Model for Robust Sequential Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657684},
doi = {10.1145/3626772.3657684},
abstract = {Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudsters are injected into the training data to manipulate learned patterns. Traditional defense methods predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attacks. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), we attempt to introduce LLMs into defense methods to broaden the knowledge beyond limited known attacks. We propose LoRec, an innovative framework that employs LLM-Enhanced Calibration to strengthen the robustness of sequential Recommender systems against poisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that refines the training process of sequential recommender systems with knowledge derived from LLMs, applying a user-wise reweighting to diminish the impact of attacks. Incorporating LLMs' open-world knowledge, the LCT effectively converts the limited, specific priors or rules into a more general pattern of fraudsters, offering improved defenses against poisons. Our comprehensive experiments validate that LoRec, as a general framework, significantly strengthens the robustness of sequential recommender systems.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1733–1742},
numpages = {10},
keywords = {large language model, poisoning attack, robust sequential recommendation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{
zhang2024understanding,
google_scholar_id={DkZNVXde3BIC},
arxiv={https://arxiv.org/pdf/2410.22844},
abbr = {NeurIPS},
title={Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation},
author={Kaike Zhang and Qi Cao and Yunfan Wu and Fei Sun and Huawei Shen and Xueqi Cheng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=k8AYft5ED1}
}


@article{Chen-2023-privacy,
abbr = {TOIS},
google_scholar_id={7Hz3ACDFbsoC},
arxiv={https://arxiv.org/pdf/2204.00279},
author = {Chen, Ziqian and Sun, Fei and Tang, Yifan and Chen, Haokun and Gao, Jinyang and Ding, Bolin},
title = {Studying the Impact of Data Disclosure Mechanism in Recommender Systems via Simulation},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3569452},
doi = {10.1145/3569452},
abstract = {Recently, privacy issues in web services that rely on users’ personal data have raised great attention. Despite that recent regulations force companies to offer choices for each user to opt-in or opt-out of data disclosure, real-world applications usually only provide an “all or nothing” binary option for users to either disclose all their data or preserve all data with the cost of no personalized service. In this article, we argue that such a binary mechanism is not optimal for both consumers and platforms. To study how different privacy mechanisms affect users’ decisions on information disclosure and how users’ decisions affect the platform’s revenue, we propose a privacy-aware recommendation framework that gives users fine control over their data. In this new framework, users can proactively control which data to disclose based on the tradeoff between anticipated privacy risks and potential utilities. Then we study the impact of different data disclosure mechanisms via simulation with reinforcement learning due to the high cost of real-world experiments. The results show that the platform mechanisms with finer split granularity and more unrestrained disclosure strategy can bring better results for both consumers and platforms than the “all or nothing” mechanism adopted by most real-world applications.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {60},
numpages = {26},
keywords = {GDPR, privacy, Recommender system}
}

@inproceedings{xie-etal-2021-factual-consistency,
    abbr = {EMNLP (Findings)},
    google_scholar_id={4X0JR2_MtJMC},
    pdf={https://aclanthology.org/2021.findings-emnlp.10},
    github={https://github.com/xieyxclack/factual_coco},
    title = "Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation",
    author = "Xie, Yuexiang  and
      Sun, Fei  and
      Deng, Yang  and
      Li, Yaliang  and
      Ding, Bolin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.10/",
    doi = "10.18653/v1/2021.findings-emnlp.10",
    pages = "100--110",
    abstract = "Despite significant progress has been achieved in text summarization, factual inconsistency in generated summaries still severely limits its practical applications. Among the key factors to ensure factual consistency, a reliable automatic evaluation metric is the first and the most crucial one. However, existing metrics either neglect the intrinsic cause of the factual inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation with human judgments or increasing the inconvenience of usage in practice. In light of these challenges, we propose a novel metric to evaluate the factual consistency in text summarization via counterfactual estimation, which formulates the causal relationship among the source document, the generated summary, and the language prior. We remove the effect of language prior, which can cause factual inconsistency, from the total causal effect on the generated summary, and provides a simple yet effective way to evaluate consistency without relying on other auxiliary tasks. We conduct a series of experiments on three public abstractive text summarization datasets, and demonstrate the advantages of the proposed metric in both improving the correlation with human judgments and the convenience of usage. The source code is available at \url{https://github.com/xieyxclack/factual_coco}."
}

@article{Wu-2022-GNN,
abbr = {CSUR},
google_scholar_id={NyGDZy8z5eUC},
pdf={https://dl.acm.org/doi/pdf/10.1145/3535101},
author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
title = {Graph Neural Networks in Recommender Systems: A Survey},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3535101},
doi = {10.1145/3535101},
abstract = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in .},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {97},
numpages = {37},
keywords = {survey, graph neural network, Recommender system}
}

@inproceedings{xie-2022-Contrastive,
  abbr = {ICDE},
  google_scholar_id={Mojj43d5GZwC},
  arxiv={https://arxiv.org/pdf/2010.14395},
  author={Xie, Xu and Sun, Fei and Liu, Zhaoyang and Wu, Shiwen and Gao, Jinyang and Zhang, Jiandong and Ding, Bolin and Cui, Bin},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Contrastive Learning for Sequential Recommendation}, 
  year={2022},
  volume={},
  number={},
  pages={1259-1273},
  keywords={Computer vision;Conferences;Multitasking;Data engineering;Data models;Behavioral sciences;Data mining;Contrastive Learning;Deep Learning;Recom-mender Systems},
  doi={10.1109/ICDE53745.2022.00099}}

@inproceedings{Chen-2022-unlearn,
abbr = {WebConf},
google_scholar_id={OP4eGU-M3BUC},
pdf={https://dl.acm.org/doi/pdf/10.1145/3485447.3511997},
github={https://github.com/chenchongthu/Recommendation-Unlearning},
author = {Chen, Chong and Sun, Fei and Zhang, Min and Ding, Bolin},
title = {Recommendation Unlearning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511997},
doi = {10.1145/3485447.3511997},
abstract = {Recommender systems provide essential web services by learning users’ personal preferences from collected data. However, in many cases, systems also need to forget some training data. From the perspective of privacy, users desire a tool to erase the impacts of their sensitive data from the trained models. From the perspective of utility, if a system’s utility is damaged by some bad data, the system needs to forget such data to regain utility. While unlearning is very important, it has not been well-considered in existing recommender systems. Although there are some researches have studied the problem of machine unlearning, existing methods can not be directly applied to recommendation as they are unable to consider the collaborative information. In this paper, we propose RecEraser, a general and efficient machine unlearning framework tailored to recommendation tasks. The main idea of RecEraser is to divide the training set into multiple shards and train submodels with these shards. Specifically, to keep the collaborative information of the data, we first design three novel data partition algorithms to divide training data into balanced groups. We then further propose an adaptive aggregation method to improve the global model utility. Experimental results on three public benchmarks show that RecEraser can not only achieve efficient unlearning but also outperform the state-of-the-art unlearning methods in terms of model utility. The source code can be found at https://github.com/chenchongthu/Recommendation-Unlearning},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2768–2777},
numpages = {10},
keywords = {Collaborative Filtering;, Machine Unlearning, Recommender Systems, Selective Deletion},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{Liu-2021-Slate,
abbr = {WebConf},
google_scholar_id={Y5dfb0dijaUC},
arxiv={https://arxiv.org/pdf/2102.13302},
github={https://github.com/CharlieMat/PivotCVAE},
author = {Liu, Shuchang and Sun, Fei and Ge, Yingqiang and Pei, Changhua and Zhang, Yongfeng},
title = {Variation Control and Evaluation for Generative Slate Recommendations},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449864},
doi = {10.1145/3442381.3449864},
abstract = {Slate recommendation generates a list of items as a whole instead of ranking each item individually, so as to better model the intra-list positional biases and item relations. In order to deal with the enormous combinatorial space of slates, recent work considers a generative solution so that a slate distribution can be directly modeled. However, we observe that such approaches—despite their proved effectiveness in computer vision—suffer from a trade-off dilemma in recommender systems: when focusing on reconstruction, they easily over-fit the data and hardly generate satisfactory recommendations; on the other hand, when focusing on satisfying the user interests, they get trapped in a few items and fail to cover the item variation in slates. In this paper, we propose to enhance the accuracy-based evaluation with slate variation metrics to estimate the stochastic behavior of generative models. We illustrate that instead of reaching to one of the two undesirable extreme cases in the dilemma, a valid generative solution resides in a narrow “elbow” region in between. And we show that item perturbation can enforce slate variation and mitigate the over-concentration of generated slates, which expand the “elbow” performance to an easy-to-find region. We further propose to separate a pivot selection phase from the generation process so that the model can apply perturbation before generation. Empirical results show that this simple modification can provide even better variance with the same level of accuracy compared to post-generation perturbation methods.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {436–448},
numpages = {13},
keywords = {Conditional Variational Auto-Encoder, Generative Recommendation, Slate Recommendation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{deng-2021-conv,
abbr = {SIGIR},
google_scholar_id={BwyfMAYsbu0C},
arxiv={https://arxiv.org/pdf/2105.09710},
github={https://github.com/dengyang17/unicorn},
author = {Deng, Yang and Li, Yaliang and Sun, Fei and Ding, Bolin and Lam, Wai},
title = {Unified Conversational Recommendation Policy Learning via Graph-based Reinforcement Learning},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462913},
doi = {10.1145/3404835.3462913},
abstract = {Conversational recommender systems (CRS) enable the traditional recommender systems to explicitly acquire user preferences towards items and attributes through interactive conversations. Reinforcement learning (RL) is widely adopted to learn conversational recommendation policies to decide what attributes to ask, which items to recommend, and when to ask or recommend, at each conversation turn. However, existing methods mainly target at solving one or two of these three decision-making problems in CRS with separated conversation and recommendation components, which restrict the scalability and generality of CRS and fall short of preserving a stable training procedure. In the light of these challenges, we propose to formulate these three decision-making problems in CRS as a unified policy learning task. In order to systematically integrate conversation and recommendation components, we develop a dynamic weighted graph based RL method to learn a policy to select the action at each conversation turn, either asking an attribute or recommending items. Further, to deal with the sample efficiency issue, we propose two action selection strategies for reducing the candidate action space according to the preference and entropy information. Experimental results on two benchmark CRS datasets and a real-world E-Commerce application show that the proposed method not only significantly outperforms state-of-the-art methods but also enhances the scalability and stability of CRS.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1431–1441},
numpages = {11},
keywords = {conversational recommendation, graph representation learning, reinforcement learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}


@article{Zhang-2020-graph,
abbr = {TOIS},
google_scholar_id={tS2w5q8j5-wC},
author = {Zhang, Yuan and Sun, Fei and Yang, Xiaoyong and Xu, Chen and Ou, Wenwu and Zhang, Yan},
title = {Graph-based Regularization on Embedding Layers for Recommendation},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3414067},
doi = {10.1145/3414067},
abstract = {Neural networks have been extensively used in recommender systems. Embedding layers are not only necessary but also crucial for neural models in recommendation as a typical discrete task. In this article, we argue that the widely used l2 regularization for normal neural layers (e.g., fully connected layers) is not ideal for embedding layers from the perspective of regularization theory in Reproducing Kernel Hilbert Space. More specifically, the l2 regularization corresponds to the inner product and the distance in the Euclidean space where correlations between discrete objects (e.g., items) are not well captured. Inspired by this observation, we propose a graph-based regularization approach to serve as a counterpart of the l2 regularization for embedding layers. The proposed regularization incurs almost no extra computational overhead especially when being trained with mini-batches. We also discuss its relationships to other approaches (namely, data augmentation, graph convolution, and joint learning) theoretically. We conducted extensive experiments on five publicly available datasets from various domains with two state-of-the-art recommendation models. Results show that given a kNN (k-nearest neighbor) graph constructed directly from training data without external information, the proposed approach significantly outperforms the l2 regularization on all the datasets and achieves more notable improvements for long-tail users and items.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {2},
numpages = {27},
keywords = {neural recommender system, graph-based regularization, Embedding}
}

@inproceedings{Lin-2019-pareto,
abbr = {RecSys},
google_scholar_id={yD5IFk8b50cC},
pdf={http://ofey.me/papers/Pareto.pdf},
award = {true},
award_name = {Best Long Paper Runner Up},
author = {Lin, Xiao and Chen, Hongjie and Pei, Changhua and Sun, Fei and Xiao, Xuanji and Sun, Hanxiao and Zhang, Yongfeng and Ou, Wenwu and Jiang, Peng},
title = {A pareto-efficient algorithm for multiple objective optimization in e-commerce recommendation},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346998},
doi = {10.1145/3298689.3346998},
abstract = {Recommendation with multiple objectives is an important but difficult problem, where the coherent difficulty lies in the possible conflicts between objectives. In this case, multi-objective optimization is expected to be Pareto efficient, where no single objective can be further improved without hurting the others. However existing approaches to Pareto efficient multi-objective recommendation still lack good theoretical guarantees.In this paper, we propose a general framework for generating Pareto efficient recommendations. Assuming that there are formal differentiable formulations for the objectives, we coordinate these objectives with a weighted aggregation. Then we propose a condition ensuring Pareto efficiency theoretically and a two-step Pareto efficient optimization algorithm. Meanwhile the algorithm can be easily adapted for Pareto Frontier generation and fair recommendation selection. We specifically apply the proposed framework on E-Commerce recommendation to optimize GMV and CTR simultaneously. Extensive online and offline experiments are conducted on the real-world E-Commerce recommender system and the results validate the Pareto efficiency of the framework.To the best of our knowledge, this work is among the first to provide a Pareto efficient framework for multi-objective recommendation with theoretical guarantees. Moreover, the framework can be applied to any other objectives with differentiable formulations and any model with gradients, which shows its strong scalability.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {20–28},
numpages = {9},
keywords = {learning to rank, multiple obecjtive optimization, pareto efficiency, recommendation},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}
@inproceedings{fei-2019-BERT4REC,
abbr = {CIKM},
google_scholar_id={RYcK_YlVTxYC},
pdf={http://ofey.me/papers/Pareto.pdf},
github={https://github.com/FeiSun/BERT4Rec},
author = {Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
title = {BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357895},
doi = {10.1145/3357384.3357895},
abstract = {Modeling users' dynamic preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks to encode users' historical interactions from left to right into hidden representations for making recommendations. Despite their effectiveness, we argue that such left-to-right unidirectional models are sub-optimal due to the limitations including: begin enumerate* [label=seriesitshapealph*upshape)] item unidirectional architectures restrict the power of hidden representation in users' behavior sequences; item they often assume a rigidly ordered sequence which is not always practical. end enumerate* To address these limitations, we proposed a sequential recommendation model called BERT4Rec, which employs the deep bidirectional self-attention to model user behavior sequences. To avoid the information leakage and efficiently train the bidirectional model, we adopt the Cloze objective to sequential recommendation, predicting the random masked items in the sequence by jointly conditioning on their left and right context. In this way, we learn a bidirectional representation model to make recommendations by allowing each item in user historical behaviors to fuse information from both left and right sides. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1441–1450},
numpages = {10},
keywords = {sequential recommendation, cloze, bidirectional sequential model},
location = {Beijing, China},
series = {CIKM '19}
}

